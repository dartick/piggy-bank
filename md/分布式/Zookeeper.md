# 一致性算法（ZAB、Paxos、Raft）

​	

​		在分布式系统中，往往都需要一个协调者管理这些分散的系统。例如dubbo需要一个协调器帮助消费者找到服务提供方，RocketMQ需要一个协调器帮助生产者和消费者找到broker。然而...这些协调器都有单点问题，如果协调器失联了相关的所有服务都受影响。

​		解决单点问题很普遍的思路就是集群化，ZK也是这么做的。但是集群之后又引入了一个新的问题：数据一致性。分布式数据一致性问题一直难以得到完美解决，CAP定理更是直接指出了追求一致性必须花费的代价。

​		ZAB协议，Paxos，Raft算法都是围绕解决单点问题和数据一致性设计的。

​		系统无法同时做到一致性（C）、可用性（A）、分区容错（P），最多只能满足其中两项，所有一致性算法都受到CAP定理的约束。

​		**分区容错**：采取分布式系统的办法解决单点故障，显然所有的分布式系统都拥有了P，那么只剩下A或者C了。

​		**一致性**：和ACID中的C不是同一个概念，这里指的是多个系统之间进行数据备份，备份结果完全相同。ACID中的C指的是数据块在业务逻辑上的一致性。[详情](https://www.jdon.com/46956) 

​		**可用性**：系统是否可以对外提供服务。高可用指的是，用户在任何时候访问系统，都可以得到系统的应答。		

​		发生数据更新时：

​		如果不想让用户看到数据同步这段时间内的不一致数据视图，就不能为用户提供服务（CP）。

​		如果不想在数据同步时影响到用户使用，用户就可能看到不同的数据视图（AP）。

​		CAP定理限制的是，无法做到在任何时候都可以访问分布式集群**并且**看到的数据视图是一致的。



​		理论上对于信道本身不可靠（信息被篡改，也叫拜占庭问题）的情况没有任何算法可以保证数据一致性。因此所有的一致性算法都假设信道是可靠的。实际上这并不难做到，分布式系统往往部署在内网中或者以加密的方式进行传输，保证了信息不被篡改。由于传输过程受到电磁干扰等影响信号的情况，通过简单的校验算法就可以避免。简而言之，Paxos解决了信道可靠情况下的分布式系统之间数据一致性问题。





## Paxos算法

​		Paxos在一致性算法中一直占有很强的影响力，它的优势只有一个（破折号）已证明是正确的。在教学和学术讨论中Paxos是常客，但是在工程实践中它有两个致命的缺点：

- 难以理解。NSDI 2012会议上，一些组织表示： 我们自己也曾深陷其中，我们在读过几篇简化它的文章并且设计了我们自己的算法之后才完全理解了 Paxos，而整个过程花费了将近一年的时间。 
- 难以实现。它对于某些问题仅仅粗略的描述了可能的方法，或提出可能的优化但是缺少实现细节（例如选主，数据同步）。一个典型的例子就是 Chubby的一条评论：Paxos 算法的描述与实际实现之间存在巨大的鸿沟…最终的系统往往建立在一个没有被证明的算法之上。 

​		Paxos算法解决的问题：它允许一组不一定可靠的处理器（服务器）在某些条件得到满足的情况下就能达成确定的安全的共识，如果条件不能满足也确保这组处理器（服务器）保持一致。 对此提出了两个概念：

​		安全性：被选定的值必须真的是被提出的值。只有一个值被选定。如果某个进程认为某个值被选定了，那么这个值必须真的是被选定的那个。

​		活性：一致总能达成。

> 对于一致性算法要保证的为什么是安全性（Safety）和活性（Liveness）我并不理解，并且二者的定义为什么要这样描述（安全性被描述成：一旦系统就某个值达成一致，便不能对另一个值再次达成一致。这条约束也很合理） 这些灵活的描述对算法的设计会有什么样的启发？对于一致性算法的约束只有这两条吗？理解接下来的Paxos过程并不困难，但是对于Paxos为什么是正确的以及作者以什么样的思路推导出的Paxos就不得而知了。

​		

​		定义了三种角色，`proposer，提交提案` ，`acceptor，批准提案，过半批准的提案会被选定` ，`learner，获取已经选定的提案` 。每个进程可以兼职多种角色。进程之间以收发消息的方式进行通信：

> 引入learner的目的是为了提高写的效率，假设集群中机器数量是固定的，不难理解减少其中的acceptor可以提高二阶段写效率。

- 进程之间以任意的速度执行，可能宕机或重启。

- 消息可能延时，重复，丢失。但是不会损坏（不存在拜占庭问题）。

​		[关于Paxos的详细介绍](https://www.jdon.com/artichect/paxos.html) 

​		Paxos的读过程：客户端会请求读取所有节点，然后从大多数节点中获取值，如果数量凑不够大多数或者没有足够的客户端响应，读取操作失败，如果返回的值都是同样的，它就算成功地读操作了 。 在现实中使用Paxos实现时，其实不需要每个节点都进行一次读取，会有更好的读取方式，但是他们都是拓展的原始 Paxos 算法。前面提到了zk保证了单一视图，也就是读操作的一致性。

​		序列号：介绍写过程之前先理解序列号的概念，每一个提案（写请求）都会跟随一个全局唯一的序列号。它由“发起建议”的节点产生，用于区分新旧提案。节点服务在接收提案时，会根据序列号丢弃旧的提案。所以保证序列号不重复是Paxos的关键之一。Paxos算法假定已经有程序产生符合条件的序列号，所以算法本身并没有要求序列号应该如何产生。

​		客户端提交新值，会随机选择一台服务发送请求，这台收到请求的节点会将请求以“建议”的形式发送给其他所有的节点。“建议”可能失败/成功。

​		第一阶段（ prepare/promise ）：简单来说就是收到客户端请求的节点，给其余所有节点发送prepare请求。其余节点接受到请求之后，如果发现序列号是自己见过最大的序列号，就回复一个promise（附带自己见过的最大的ID），承诺自己接下来不会接受任何比此序列号更小的请求。发出prepare的节点在某个时间段内收集到足够多的promise之后，便认为自己获得了发言权，进行下一阶段。否则此建议会被丢弃，客户端写入失败。这里可以有一个优化，如果任何prepare发现了比此次提案编号更高的提案，就可以提醒“建议”节点中断此次请求。记住一点，某个值将要被提交在第一阶段就已经被确定了。

​		第二阶段（ propose/accept ）：“建议”节点收集到足够多的的promise之后，会向其余所有节点发送propose 这是一个“确认commit”阶段。只要propose的序列号没有小于acceptor的promise，acceptor就会批准它，**本地持久化**之后返回。如果没有冲突建议、失败或分区错误，那么这个新建议将被所有其他节点接受，那么Paxos过程就完成了。 

> 读写过程就是Paxos算法的核心内容，读就是多读几个服务器，写就是两阶段写。并不像网上说的那样Paxos难以理解，相反它看起来很简单。其实Paxos说的难并不是指最后具体的这个过程，难点在如何证明此过程在所有它保证的情况下都可以正确工作。不幸的是我现在也没看懂Paxos的证明过程，只能这里贴几个相关传送门。[知乎](https://zhuanlan.zhihu.com/p/31780743)  [知乎](https://www.zhihu.com/question/19787937) [维基百科](https://zh.wikipedia.org/zh-hans/Paxos%E7%AE%97%E6%B3%95) [Paxos Made Simple](http://research.microsoft.com/users/lamport/pubs/paxos-simple.pdf) 

​		Learning的发现策略：

​		learner发现过半acceptor批准了提案，才会学习这个提案。最显然的发现算法就是让每个acceptor，只要批准一个提案都会把这个消息发送给所有的learner。但是这样造成太多不必要的通讯次数。在非拜占庭问题前提下，一个learner从另一个learner那学习知识是很简单的。我们可以让acceptor在接受提案时和其中一个learner通讯，然后再由这个learner通知其他的learner，这种方案减少了通讯次数但是存在单点问题。

​		通常的办法是，acceptor在接受提案后告诉一组learner，每个learner在收到消息之后都会通知剩余所有的learner。因为存在消息丢失，learner可以询问acceptor他们接收了哪个提案。但是acceptor的宕机会导致没有超过半数的机器回答询问。这种情况下learner只在新的提案被接受时才问询哪个提案被接收了。如果learner想知道某个值是否被选择了，它可以让proposer使用上述算法提出一个提案。



#### 		演进:Multi-Paxos

​		按照上述算法，很容易构造出一个场景：两个proposer持续提交提案但是没有提案被选定。Proposer P提交一个提案序列号为n1并且完成了阶段1。Proposer q紧接着完成了序列号n2>n1的提案。p的n1的第二阶段会被忽略，因为n2>n1acceptor承诺不再接受小于n2的提案。于是p开始提交n3并完成了第一阶段.对于n2来说n3就相当于自己对于n1于是n2被拒绝。q开始提交n4，这个过程可以一直运行而任何提案都没有被接受。这被称为活锁。

​		为了避免上述过程，必须规定只有一个特殊的proposer提交提案，被称为leader。它将决定某个来自客户端的提案的序列号并提交。这个设定下，prepare阶段可以省略。leader自增序列号后直接发送propose，通常情况会成功，但是在故障情况或者别的proposer认为自己也是leader的情况下会失败。只要选举唯一proposer的算法能够正确，整个系统就可以正确运行，神说：“选举一个proposer必须是实时或随机的”。

​		

​		如何选举leader？

​		Paxos算法并没有约束如何选举leader，可以根据自己的系统灵活定义。这里举例一种 Leslie Lamport 提出的方案，后续也会看到raft是如何选举leader的。

- 让有 **最高 ID** 的服务器作为领导者
- 可以通过每个服务器定期向其他服务器 **发送心跳消息** 的方式来实现。这些消息包含发送服务器的 ID 
- 如果它们没有能收到某一具有高 ID 的服务器的心跳消息，然后它们会自己选举成为领导者。 
- 也就是说，首先它会从客户端接受到请求，其次在 Paxos 协议中，它会 **同时扮演 proposer 和 acceptor** 
- 如果机器能够接收到来自高 ID 的服务器的心跳消息，它就不会作为 leader，如果它接收到客户端的请求，那么它会 **拒绝** 这个请求，并告知客户端与 leader 进行通信。 
- 非 leader 服务器不会作为 proposer，**只会作为 acceptor** 。
-  这个机制的优势在于，它不太可能出现两个 leader 同时工作的情况，即使这样，如果出现了两个 leader，Paxos 协议还是能正常工作，此时退化成Basic-Paxos。
-  应该注意的是，实际上大多数系统都不会采用这种选举方式，它们会采用基于 **租约** 的方式（lease based approach），这比上述介绍的机制要复杂的多，不过也有其优势。 



​		选举新的leader时，如何进行数据同步？

​		新的leader诞生，它自己的数据不一定是最高版本的需要同步。

​		其他的acceptor也存在同样的问题，如何解决？



>  http://www.yeolar.com/note/2013/09/18/paxos-simple/  这里有论文原文的翻译，看完也不知道上面的问题是怎么解决的。文章开头提到的书中也没有任何相关介绍。我放弃了，看下面的ZAB协议中如何解决此问题的吧。

​	





## Raft

​		Raft是针对Paxos**难以理解**和**难以实现**提出的一致性算法。吸取了Paxos的思想，除此之外做了更多算法细节上的工作以保证参考Raft算法可以快速实现一个共识系统。一下内容基本都是参考[这篇](https://www.infoq.cn/article/raft-paper)论文的译文总结的。

​		在保证可用的基础上，它提供了完整的，更贴合实际的基础。更重要的是它保证大多数人能够直观的理解整个算法，在某些问题面临的诸多解决方案中raft往往采取最易于理解的一种。它也会尽可能的根据实际情况考虑问题，例如理论上某些场景可以有一些优化策略，但是考虑实际场景发生的概率，可能会放弃优化而简化系统复杂度。raft的设计思路有两条：

- 问题拆解。 把问题分解成为了**领导选取（leader election）**、**日志复制（log replication）**、**安全（safety）**和**成员变化（membership changes）** 。
- 尽可能消除不确定性 。通过减少需要考虑的状态的数量将状态空间简化，这能够使得整个系统更加一致。具体做法是增加约束，例如：成员变动，只允许一次变动一个成员而不是任意数量的成员。 日志之间不允许出现空洞 。领导人必须拥有编号最高的日志。



Raft的语义和Paxos，ZAB基本类似：

- 选举安全原则（Election Safety）：一个任期（term）内最多允许有一个领导人被选上 
- 领导人只增加原则（Leader Append-Only）：领导人永远不会覆盖或者删除自己的日志，它只会增加条目 
- 日志匹配原则（Log Matching）：如果两个日志在相同的索引位置上的日志条目的任期号相同，那么我们就认为这个日志从头到这个索引位置之间的条目完全相同 
- 领导人完全原则（Leader Completeness)：如果一个日志条目在一个给定任期内被提交，那么这个条目一定会出现在所有任期号更大的领导人中
- 状态机安全原则（State Machine Safety）：如果一个服务器已经将给定索引位置的日志条目应用到状态机中，则所有其他服务器不会在该索引位置应用不同的条目

 



### 消息广播

​		基本的RPC有两种：RequestVote RPC是候选人在选举过程中触发的 。 AppendEntries RPC 是领导人在复制日志和触发的。

还有一种快照RPC为了提高性能。

​		写操作和ZAB，Paxos一样也是个2PC过程。rate中的每个服务都维护了一个日志索引（log index），保证所有服务的相同日志索引处的日志是一致的（或者说最终一致），这也叫日志匹配原则。日志匹配原则需要这样一个机制完成：

​		每个日志除了携带term编号，还会携带日志索引和 之前的条目的索引位置和term编号 。如果追随者没有在它的日志中找到相同索引和任期号的日志，它就会拒绝新的日志条目。这个一致性检查就像一个归纳步骤：一开始空的日志的状态一定是满足日志匹配原则的，一致性检查保证了当日志添加时的日志匹配原则。因此，只要 AppendEntries 返回成功的时候，领导人就知道追随者们的日志和它的是一致的了。 

​		对于follower和leader日志不一致的情况，raft强制让follower复制leader的日志进行同步。具体的做法是leader为每个follower维护一个nextIndex表示leader将要发送的下一个日志的索引，发送的 AppendEntries RPC一致性检查失败会返回false，此时将nextIndex递减，将更早的日志封装成 AppendEntries RPC 发送出去。这样就达成了从前往后，直到日志相同的地方就可以依次同步了。 



### 选举

​		在所有以leader为基础的一致性算法中，必须保证leader拥有全部的数据。在某些一致性算法中，即使被选举的leader刚开始并不具备全部数据，也会在正式广播数据前完成数据同步。但是这种方式让系统复杂度变得很高。所以ZAB和Raft，都采取了相同的限制来规避这种复杂：规定只有拥有最高编号（意味着全部数据）日志的follower才有资格当选leader。

​		具体的选举过程：触发选举的条件是follower超过t时间（每个follower的时间是固定间隔内的随机时间）未检测到leader的心跳，follower角色转变为 Candidate，开始号票。收到候选者号票的follower，会比较对方和自己的日志编号，如果对方小于自己会拒绝投票。在每个任期中，follower只能投一票，这会造成某个任期没有节点被选为leader，会在超时时间之后触发下一轮选举。



### 日志压缩

此功能有两个目的：减少日志堆积节约空间，快照之前的日志都可以删除。减少系统重启的回放压力，当某follower崩溃很久才恢复或者新加入的follower，可以从快照点开始同步数据而不需要同步全部历史数据。

系统选择某个点建立日志快照。快照内容包括：当前日志信息，当前系统状态。



### 成员变更

​		成员变更，也是一种一致性问题。成员变更特殊在会影响所有投票的过程，所以不能看做一般的一致性问题处理。为了解决这个问题，raft提出两阶段变更。

​		Raft把旧的成员配置成为C-old，新的成员配置成为C-new。并且定义了变更的中间状态配置： Cold U Cnew.每个节点都有一份成员配置。变更过程如下：

1. Leader收到成员变更请求从Cold切成Cnew；
2. Leader在本地生成一个新的log entry，其内容是Cold∪Cnew，代表当前时刻新旧成员配置共存，写入本地日志，同时将该log entry复制至Cold∪Cnew中的所有副本。在此之后新的日志同步需要保证得到Cold和Cnew两个多数派的确认；
3. Follower收到Cold∪Cnew的log entry后更新本地日志，并且此时就以该配置作为自己的成员配置；
4. 如果Cold和Cnew中的两个多数派确认了Cold U Cnew这条日志，Leader就提交这条log entry；
5. 接下来Leader生成一条新的log entry，其内容是新成员配置Cnew，同样将该log entry写入本地日志，同时复制到Follower上；
6. Follower收到新成员配置Cnew后，将其写入日志，并且从此刻起，就以该配置作为自己的成员配置，并且如果发现自己不在Cnew这个成员配置中会自动退出；
7. Leader收到Cnew的多数派确认后，表示成员变更成功，后续的日志只要得到Cnew多数派确认即可。Leader给客户端回复成员变更执行成功。



异常情况分析：

- 如果Leader的Cold U Cnew尚未推送到Follower，Leader就挂了，此后选出的新Leader并不包含这条日志，此时新Leader依然使用Cold作为自己的成员配置。
- 如果Leader的Cold U Cnew推送到大部分的Follower后就挂了，此后选出的新Leader可能是Cold也可能是Cnew中的某个Follower。
- 如果Leader在推送Cnew配置的过程中挂了，那么同样，新选出来的Leader可能是Cold也可能是Cnew中的某一个，此后客户端继续执行一次改变配置的命令即可。
- 如果大多数的Follower确认了Cnew这个消息后，那么接下来即使Leader挂了，新选出来的Leader肯定位于Cnew中。



以上的异常情况实现起来太复杂，raft通过增加成员变更的约束条件来使情况变得简单。具体来说就是规定：每次只能变动一个节点。



### 客户端交互

主要内容：读、写、发现leader。

发现leader：在Raft中读写操作都是通过leader完成的。客户端首先连接任意一台机器，如果这台机器是follower，follower会将自己所知道的leader信息返回给客户端。

读：需要保证读到的是最新的数据，分为两种情况。

- leader当选之初可能不知道当前被提交日志的情况，因为此刻leader可能保存了上任期接收到但是并未提交的日志。此时leader需要通过follower的信息来确认这条日志有没有被提交，然后才能响应客户端的读操作。
- leader随时可能已经脱离组织但是自己还不知道。所以响应请求之前会先和大多数节点进行一次心跳。

写：交互过程中需要解决的地方是，leader没来得及返回结果就宕机了，正常情况下客户端很可能选择重新发送这条指令。这样会导致指令被执行了两次，可以认为执行两次是被允许的。但是也有解决方案：让每条指令都带一个UUID，leader如果发现执行过了就可以直接返回成功。 





## Zookeeper

 	关于Zookeeper的详细介绍，有一本公认还不错的书《[从Paxos到Zookeeper 分布式一致性原理与实践]([https://github.com/double-qiu/books/blob/master/%E4%BB%8EPAXOS%E5%88%B0ZOOKEEPER%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8E%9F%E7%90%86%E4%B8%8E%E5%AE%9E%E8%B7%B5.pdf](https://github.com/double-qiu/books/blob/master/从PAXOS到ZOOKEEPER分布式一致性原理与实践.pdf))》，但只看这本书基本上看不懂Zookeeper，前几章对于2PC，3PC讲解的还算清楚。如果只看这本书大概率是看不懂Paxos和Zookeeper的。推荐观赏 [Apache Zookeeper](https://zookeeper.apache.org/doc/r3.5.6/zookeeperOver.html) .







### ZK的功能

ZK保证了：

- 顺序一致性，主要是写顺序的一致性，**同一个**客户端的写会按照严格的顺序执行。

- 原子性：集群中的所有机器要么都提交了某事物，要么都没提交某事物。

- 单一视图：任何客户端连接到集群中的任何机器，看到的数据都是一样的。

- 可靠性：可以理解为关系型数据库中持久性

- 实时性：发生数据更新后，保证在一段确定的时间内客户端能够看到更新.




​	基于这些基本的功能特性，业界可以用ZK来做：数据的发布/订阅，负载均衡，命名服务，分布式协调/通知，集群管理，Master选举，分布式锁，分布式队列。





### ZK的实现原理

​		ZK并没有完整的实现Paxos算法，而是参考了Paxos中的思想设计了ZAB协议。后续的介绍中可以体会ZAB与Paxos的不同/相同之处。

#### 		角色：

- leader：为客户端提供读/写功能。ZK中只有一个Leader角色，所有的数据写操作都由leader完成。
- follower：提供读服务，参与leader选举，参与“过半写”策略
- observer：提供读服务，不参与任何投票

#### 		会话：

​		客户端启动时会和其中一台ZK建立TCP长连接，标志着会话开始。可以通过这个连接发送心跳检测，提交数据，读取数据，接受服务器的通知。会话有一个Session Timeout时间，如果客户端由于各种原因（网络扰动，主动断开等）超过Timeout时间没连会来，就标志着会话结束。

#### 		Znode：

​		树结构，分为临时节点和永久节点。永久阶段除非主动remove否则会一直存在，临时阶段和会话绑定随着会话结束消失。以dubbo中对zk的使用为例。永久节点通常存放系统名称，临时节点则存放具体的系统实例，会随着系统扩容，重启，宕机等状态改变而改变。



#### 版本：

​		version：当前ZNode的版本。cversion：当前ZNode子节点的版本。aversion：当前ZNode的ACL版本。

​		以上是文档的说法，实际上webtool上看到的，并不是这样。



#### Watcher：

​		事件监听器。 

- 主动推送：Watch被触发时，由 ZooKeeper 服务器主动将更新推送给客户端，而不需要客户端轮询。
- 一次性：数据变化时，Watch 只会被触发一次。如果客户端想得到后续更新的通知，必须要在 Watch 被触发后重新注册一个 Watch。
- 可见性：如果一个客户端在读请求中附带 Watch，Watch 被触发的同时再次读取数据，客户端在得到 Watch 消息之前肯定不可能看到更新后的数据。换句话说，更新通知先于更新结果。
- 顺序性：如果多个更新触发了多个 Watch ，那 Watch 被触发的顺序与更新顺序一致。



#### ACL:

- CREATE:创建子节点。
- READ：获取子借点数据和子节点列表。
- WRITE：更新节点数据。
- DELETE：删除子节点。
- ADMIN：设置节点的ACL权限。



#### 		ZAB协议：

​		ZAB协议也是一套理论，有多种版本多种语言的具体实现，Zookeeper的Java版本只是其中的一种。ZAB不是一个通用的一致性方案，它采纳了Paxos的思想。ZAB采用了唯一一个leader进行写操作，并且保证同一时刻集群中只会存在一个leader。但是事务的提交依然分为两阶段完成。相比PaxosZAB考虑了更多实际工程上的问题，比如要考虑数据之间的依赖关系，因此所有的事务必须严格的按照客户端的提交顺序执行、增加了动态扩容等机制、考虑了高吞吐量场景等。整个ZAB协议分为消息广播和崩溃恢复两个模式。

​		消息广播：正常提供服务时的模式，提供数据读写。

​		崩溃恢复：系统启动或leader崩溃时会进入此模式，新加入的节点自身也会进入此模式。完成leader选举，follower与leader数据同步两件事。

​		

ZAB协议为进程定义了三种状态：LOOKING选举阶段，FOLLOWING广播阶段的正常状态，LEADING领导状态。

定义了ZXID：用作全局事务的ID，64位，高32位表示当前leader的届数也叫epoch，每成功选举一个新的leader就+1。低32位表示当前leader提交的事务编号，每提交一个事务就+1。



##### 		阶段0：选举

​		ZAB协议并没有规定如何选举。选举leader需要考虑到后面的数据同步，一种高效的选举方案是让拥有最大ZXID的服务当选leader。这样可以保证，新选举出的leader一定知道全部已提交的事务。省去新leader检查历史事务提交和丢弃情况。

​	Fast Leader Election：规则很简单，按照epoch，zxid，myid，排序选择最大的。

​	**大致过程**： 节点在选举开始都默认投票给自己，当接收其他节点的选票时，会根据上面的条件更改自己的选票并重新发送选票给其他节点，当有一个节点的得票超过半数，该节点会设置自己的状态为 leading，其他节点会设置自己的状态为 following。 

​	**具体过程**（[参考文章](https://dbaplus.cn/news-141-1875-1.html))：

​		myid：这是每个zk都会有的唯一的id，每个服务的配置文件中都有其他服务的myid。

​		状态：ZAB协议为进程定义了三种，`LOOKING选举阶段`，`FOLLOWING广播阶段的正常状态`，`LEADING领导状态`。

​		zxid：用作全局事务的ID，64位，高32位表示当前leader的届数也叫epoch，每成功选举一个新的leader就+1。低32位表示当前leader提交的事务编号，每提交一个事务就+1。

​		投票箱：每个节点都会维护一个投票箱，其中记录的是所有节点的投票结果。例如 服务器2投票给服务器3，服务器3投票给服务器1，投票箱为(2, 3), (3, 1), (1, 1)。 投票的状态可能会改变（后面会介绍如何发生改变），例如服务器3改为投票给服务器4后，箱子中的数据为(2, 3), (3, 4), (1, 1)。可能由于网络分区某个投票箱中的数据不完整。

​		选票数据结构：

-  **logicClock** 每个服务器会维护一个自增的整数，名为logicClock，它表示这是该服务器发起的第多少轮投票 
-  **state** 当前服务器的状态
-  **self_id** 当前服务器的myid
-  **self_zxid** 当前服务器上所保存的数据的最大zxid
-  **vote_id** 被推举的服务器的myid
-  **vote_zxid** 被推举的服务器上所保存的数据的最大zxid

​		投票轮次：

- **自增选举轮次** 

- **初始化选票**  每个服务器在广播自己的选票前，会将自己的投票箱清空 

- **发送初始化选票** 每个服务器最开始都是通过广播把票投给自己。

- **接收外部投票**  服务器会尝试从其它服务器获取投票，并记入自己的投票箱内。如果无法获取任何外部投票，则会确认自己是否与集群中其它服务器保持着有效连接。如果是，则再次发送自己的投票；如果否，则马上与之建立连接 

- **判断选举轮次** 收到外部投票后，首先会根据投票信息中所包含的logicClock来进行不同处理 

  -  外部投票的logicClock大于自己的logicClock。说明该服务器的选举轮次落后于其它服务器的选举轮次，立即清空自己的投票箱并将自己的logicClock更新为收到的logicClock，然后再对比自己之前的投票与收到的投票以确定是否需要变更自己的投票，最终再次将自己的投票广播出去 。 
  -  外部投票的logicClock小于自己的logicClock。当前服务器直接忽略该投票，继续处理下一个投票。
  -  外部投票的logicClock与自己的相等。当时进行选票PK。

  > 轮次（logicClock）大的优先级高于轮次小的

- **选票PK** ： 基于(self_id, self_zxid)与(vote_id, vote_zxid)的对比 

  - 外部投票的logicClock大于自己的logicClock，则将自己的logicClock及自己的选票的logicClock变更为收到的logicClock
  - 若logicClock一致，则对比二者的vote_zxid，若外部投票的vote_zxid比较大，则将自己的票中的vote_zxid与vote_myid更新为收到的票中的vote_zxid与vote_myid并广播出去，另外将收到的票及自己更新后的票放入自己的票箱。如果票箱内已存在(self_myid, self_zxid)相同的选票，则直接覆盖 
  - 若二者vote_zxid一致，则比较二者的vote_myid，若外部投票的vote_myid比较大，则将自己的票中的vote_myid更新为收到的票中的vote_myid并广播出去，另外将收到的票及自己更新后的票放入自己的票箱 

  > 选举会选出zxid最大的，当zxid一样大的时候就比较myid,约定此时谁名字大听谁的，理论上也可以约定谁名字小听谁的。

- **统计选票** ：如果已经确定有过半服务器认可了自己的投票（可能是更新后的投票），则终止投票。否则继续接收其它服务器的投票。

- **更新服务器状态** ：投票终止后，服务器开始更新自身状态。若过半的票投给了自己，则将自己的服务器状态更新为LEADING，否则将自己的状态更新为FOLLOWING。

##### 		阶段1：发现

​		让所有的follower获取新leader（此时的leader还不完备因此也叫准leader）的迭代数据。①所有的follower将自己当前的epoch发送给leader②当leader收到过半的epoch后，会选择其中最大的epoch并+1，开启新纪元。③follower接收到epoch+1之后，与自己当前的epoch比较，如果自己的比较小就更新为新值同时返回ACK。ACK中还包含了follower自己历史事务proposal集合$$f$$。④收到过半ACK后，leader会从ACK中挑选一个$$f$$作为整个集群同步数据的基础。此$$f$$ 的ZXID必须是全局最大的。

> 步骤②中接收到了过半就选择最大的epoch并进行步骤③，此时选择的epoch可能不是整个集群中最大的吗？其实很容易证明，过半机器必然存在与上一任leader正常通讯的机器，这台机器的epoch就是要找的那个。

##### 		阶段2：同步

​		上一步选择的$$f$$ 在这一阶段会被同步给其他所有的follower。

​		①$$f$$会被发送给集群中的所有follower。②follower在接收到数据后，会比较epoch，如果epoch不相等（正常情况下阶段1中更新为相等），表明自己还未参与过阶段1甚至上一年代的阶段1，不能接受此次数据，它将重新开始阶段1。如果epoch相等，就会将$$f$$中的数据无脑应用到自身。并在结束后返回一个ACK.。③leader收到过半ACK后向所有follower发送commit。同步阶段完成。

> follower与leader数据同步阶段，leader需要保证每一个follower都已经明确知道已经提交的所有事物。具体的，leader会为每个follower准备一个队列，并将follower没有同步的数据以proposal的形式放入队列中，这条proposal会立即跟随一个commit表示此事务已经被提交。follower执行所有的消息之后，leader会将它加入可用follower列表中，表示数据同步完成。

##### 		阶段3：广播

​		广播阶段和Paxos中的思想基本一样，只是ZAB认为事务之间可能存在依赖，必须严格按照提交的顺序执行。另外，leader会为每个与自己建立稳定连接的follower维护一个队列用于存放事务数据。

​		①客户端的所有请求都经过leader，leader生成对应事务的proposal向所有Follower发送事务请求。

​		②follower收到proposal后悔将信息追加到本地事务日志中，之后返回一个ACK。

​		③当leader接收到过半ACK，会发送Commit要求提交事务，同时自己提交事务。

​		④收到commit的follower会按照要求提交事务，但此时一定会校验事务的顺序执行，不会跳过某个事务。



ZAB协议需要保证：

​		①：已经在leader上提交的事务最终会被所有服务器提交

​		②：丢弃只在leader上提交的事务（未发送任何Proposal）



ZK为奇数台：首先应该意识到ZK部署多台，是为了防止单点故障——某些机器故障整个集群依然可以对外提供服务。例如部署5台，此时最多可以挂2台，如果部署6台呢？依然最多允许挂2台，多出来的这个机器在防止单点故障上没有任何用处。